"""
Abrar Yaser
CS51
03/07/2022
"""

from cs51neural import *

# Practice in lab

def ex1():
    xor = [([0, 0], [0]), ([0, 1], [1]), ([1, 0], [1]), ([1, 1], [0])]

    # create a neural network
    nn = NeuralNet(2, 3, 1)

    # train the nn model
    nn.train(xor)
    print("evaluate:")
    print(nn.evaluate([1, 0]))

    nn.test(xor)
    for triple in nn.test(xor):
        (input, label, prediction) = triple  # unpack the triple
        # remember outputs are lists, so we need to get the first item from the list
        print(str(input) + "\t" + str(label[0]) + "\t" + str(prediction[0]))

    nn.train(xor, iterations=1000, print_interval=200)
    print("evaluate:")
    print(nn.evaluate([1, 0]))

"""
With three hidden notes, it takes the neural network 600 tries to even get even get to an error of 0.04.
The network needs to be trained a few times for the differences to be miniscule and for the error to 
converge to zero. When I increased print_interval, the difference between the first error interval and the second 
is 0.000405, which is huge compared to the rest of the error intervals.
"""


def ex2():
    xor = [([0, 0], [0]), ([0, 1], [1]), ([1, 0], [1]), ([1, 1], [0])]

    # create a neural network
    nn = NeuralNet(2, 8, 1)

    # train the nn model
    nn.train(xor)
    print("evaluate:")
    print(nn.evaluate([1, 0]))

    nn.train(xor, iterations=1000, print_interval=200)
    print("evaluate:")
    print(nn.evaluate([1, 0]))


"""
The convergence is much faster this time because after only 400 iterations, the error was reduced from 5.383919
to 0.030817. Yes, it is a better approximation because when I evaluate the network, the value is closer to 1 than the 
first experiment with only 3 hidden nodes.
"""


def ex3():

    xor = [([0, 0], [0]), ([0, 1], [1]), ([1, 0], [1]), ([1, 1], [0])]

    # create a neural network
    nn = NeuralNet(2, 1, 1)

    # train the nn model
    nn.train(xor)
    print("evaluate:")
    print(nn.evaluate([1, 0]))

    nn.train(xor, iterations=1000, print_interval=200)
    print("evaluate:")
    print(nn.evaluate([1, 0]))


"""
The XOR function cannot be computer with a single node because the XOR function is not linearly separable. So, 
one perceptron will not be enough to meet the threshold. So, instead of one hidden node, we would need multiple so that
it can test out inputs with multiple weights. A network with one hidden node is equivalent to a single node
because each input has only one weight.
"""


def ex4():

    xor = [([0.9, 0.6, 0.8, 0.3, 0.1], [1.0]), ([0.7, 0.2, 0.4, 0.6, 0.3], [1.0]), ([0.5, 0.5, 0.8, 0.4, 0.8], [0.0]),
           ([0.3, 0.1, 0.6, 0.8, 0.8], [0.0]), ([0.6, 0.3, 0.4, 0.3, 0.6], [0.0])]

    # create a neural network
    nn = NeuralNet(5, 30, 1)

    # train the nn model

    nn.train(xor, iterations=5000, print_interval=500)

    # evaluating the party for the second entry on the table
    print("evaluate:")
    print(nn.evaluate([0.8, 0.8, 0.4, 0.6, 0.4]))

"""
I wrote a neural network that takes 5 inputs, has 30 hidden nodes, and spits out 1 output. The, I fluctuated
the number of hidden nodes to see what causes the error to converge to 0 the fastest. Finally, I decided on
30 hidden nodes. Also, I increased the iterations of the training data from 1000 to 5000, and the error converged
to zero in less time. I also tested data from the second entry on the table, and my neural network correctly predicted 
the person to be a democrat.
"""
